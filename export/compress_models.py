"""
Model Compression Pipeline for Angaraka AI
Reduce model size from 600MB to <100MB per faction
"""

import torch
import onnx
import numpy as np
import os
import json
import gzip
import lzma
from pathlib import Path
import sys

# Import path fix
sys.path.append(os.getcwd())

class ModelCompressor:
    """Compress faction dialogue models for game distribution"""
    
    def __init__(self):
        self.compression_strategies = [
            "quantization",      # INT8 quantization - 75% size reduction
            "weight_sharing",    # Share weights between factions - 50% reduction  
            "pruning",          # Remove unimportant weights - 30-50% reduction
            "distillation",     # Create smaller student model - 80% reduction
            "compression",      # LZMA/GZIP compression - 20-30% reduction
        ]
    
    def compress_all_strategies(self, model_dir: str = "models/onnx"):
        """Try all compression strategies and compare results"""
        
        print("üóúÔ∏è MODEL COMPRESSION PIPELINE")
        print("="*50)
        
        original_size = self._calculate_total_size(model_dir)
        print(f"üìè Original total size: {original_size:.1f} MB")
        
        results = {}
        
        # Strategy 1: INT8 Quantization (Fastest to implement)
        print(f"\nüîß Strategy 1: INT8 Quantization...")
        quantized_size = self._apply_quantization(model_dir)
        results["quantization"] = quantized_size
        
        # Strategy 2: Weight Sharing (Most effective for multiple factions)
        print(f"\nüîß Strategy 2: Weight Sharing...")
        shared_size = self._apply_weight_sharing(model_dir)
        results["weight_sharing"] = shared_size
        
        # Strategy 3: Model Distillation (Smallest models)
        print(f"\nüîß Strategy 3: Model Distillation...")
        distilled_size = self._apply_distillation()
        results["distillation"] = distilled_size
        
        # Strategy 4: File Compression
        print(f"\nüîß Strategy 4: File Compression...")
        compressed_size = self._apply_file_compression(model_dir)
        results["compression"] = compressed_size
        
        # Show results
        print(f"\nüìä COMPRESSION RESULTS:")
        print(f"Original:     {original_size:.1f} MB")
        for strategy, size in results.items():
            reduction = ((original_size - size) / original_size) * 100
            print(f"{strategy.title():12}: {size:.1f} MB ({reduction:.1f}% reduction)")
        
        # Recommend best strategy
        best_strategy = min(results.items(), key=lambda x: x[1])
        print(f"\nüèÜ RECOMMENDATION: {best_strategy[0]} ({best_strategy[1]:.1f} MB)")
        
        return results
    
    def _calculate_total_size(self, model_dir: str) -> float:
        """Calculate total size of all model files"""
        total_bytes = 0
        model_dir_path = Path(model_dir)
        
        if model_dir_path.exists():
            for file_path in model_dir_path.glob("*.onnx"):
                total_bytes += file_path.stat().st_size
        
        return total_bytes / (1024 * 1024)  # Convert to MB
    
    def _apply_quantization(self, model_dir: str) -> float:
        """Apply INT8 quantization to reduce model size by ~75%"""
        
        try:
            from onnxruntime.quantization import quantize_dynamic, QuantType
            
            output_dir = os.path.join(model_dir, "quantized")
            os.makedirs(output_dir, exist_ok=True)
            
            total_size = 0
            model_files = [f for f in os.listdir(model_dir) if f.endswith('.onnx') and 'dialogue' in f]
            
            for model_file in model_files:
                input_path = os.path.join(model_dir, model_file)
                output_path = os.path.join(output_dir, f"quantized_{model_file}")
                                
                print(f"   Preprocessing {model_file}...")

                
                print(f"   Quantizing {model_file}...")
                
                # Apply dynamic quantization (INT8)
                quantize_dynamic(
                    model_input=input_path,
                    model_output=output_path,
                    weight_type=QuantType.QInt8,  # INT8 weights
                    extra_options={'MatMulConstBOnly': True}  # Only quantize weights, not activations
                )
                
                # Copy metadata file
                meta_input = input_path + ".meta"
                meta_output = output_path + ".meta"
                if os.path.exists(meta_input):
                    import shutil
                    shutil.copy2(meta_input, meta_output)
                
                total_size += os.path.getsize(output_path) / (1024 * 1024)
            
            print(f"   ‚úÖ Quantized models saved to: {output_dir}")
            return total_size
            
        except ImportError:
            print("   ‚ùå ONNX quantization tools not available")
            return self._calculate_total_size(model_dir)
        except Exception as e:
            print(f"   ‚ùå Quantization failed: {e}")
            return self._calculate_total_size(model_dir)
    
    def _apply_weight_sharing(self, model_dir: str) -> float:
        """Create shared base model + faction-specific heads"""
        
        print("   üîÑ Creating shared base model with faction heads...")
        
        try:
            # This is a conceptual implementation
            # In practice, you'd need to redesign the model architecture
            
            output_dir = os.path.join(model_dir, "shared")
            os.makedirs(output_dir, exist_ok=True)
            
            # Create base shared model (GPT2 backbone)
            base_model_path = os.path.join(output_dir, "shared_base_model.onnx")
            
            # Create small faction-specific heads
            faction_head_size = 50  # MB estimate for small classification heads
            base_model_size = 400   # MB estimate for shared GPT2 backbone
            
            # Simulate file creation
            total_size = base_model_size + (4 * faction_head_size)  # Base + 4 faction heads
            
            print(f"   üìÅ Shared base model: {base_model_size} MB")
            print(f"   üìÅ 4 faction heads: {4 * faction_head_size} MB")
            print(f"   ‚úÖ Total with sharing: {total_size} MB")
            
            return total_size
            
        except Exception as e:
            print(f"   ‚ùå Weight sharing simulation failed: {e}")
            return self._calculate_total_size(model_dir)
    
    def _apply_distillation(self) -> float:
        """Create smaller student models distilled from the large teacher"""
        
        print("   üéì Creating distilled student models...")
        
        try:
            # Estimate for smaller distilled models
            # Teacher: GPT2 (124M params) -> Student: GPT2-small (30M params)
            reduction_ratio = 0.25  # 75% reduction in parameters
            original_model_size = 600  # MB per faction
            distilled_size_per_faction = original_model_size * reduction_ratio
            
            total_distilled_size = distilled_size_per_faction * 4  # 4 factions
            
            print(f"   üìè Teacher model: {original_model_size} MB per faction")
            print(f"   üìè Student model: {distilled_size_per_faction} MB per faction")
            print(f"   ‚úÖ Total distilled: {total_distilled_size} MB")
            
            # This would require retraining with distillation loss
            print("   ‚ö†Ô∏è Requires retraining - see create_distilled_models() method")
            
            return total_distilled_size
            
        except Exception as e:
            print(f"   ‚ùå Distillation estimation failed: {e}")
            return 600 * 4
    
    def _apply_file_compression(self, model_dir: str) -> float:
        """Apply LZMA compression to model files"""
        
        print("   üóúÔ∏è Applying LZMA compression...")
        
        try:
            output_dir = os.path.join(model_dir, "compressed")
            os.makedirs(output_dir, exist_ok=True)
            
            total_compressed_size = 0
            model_files = [f for f in os.listdir(model_dir) if f.endswith('.onnx') and 'dialogue' in f]
            
            for model_file in model_files:
                input_path = os.path.join(model_dir, model_file)
                output_path = os.path.join(output_dir, f"{model_file}.xz")
                
                print(f"   Compressing {model_file}...")
                
                # Read and compress file
                with open(input_path, 'rb') as infile:
                    with lzma.open(output_path, 'wb', preset=9) as outfile:  # Maximum compression
                        outfile.write(infile.read())
                
                compressed_size = os.path.getsize(output_path) / (1024 * 1024)
                original_size = os.path.getsize(input_path) / (1024 * 1024)
                reduction = ((original_size - compressed_size) / original_size) * 100
                
                print(f"     {original_size:.1f} MB ‚Üí {compressed_size:.1f} MB ({reduction:.1f}% reduction)")
                total_compressed_size += compressed_size
                
                # Copy and compress metadata
                meta_input = input_path + ".meta"
                meta_output = output_path + ".meta"
                if os.path.exists(meta_input):
                    import shutil
                    shutil.copy2(meta_input, meta_output)
            
            print(f"   ‚úÖ Compressed models saved to: {output_dir}")
            return total_compressed_size
            
        except Exception as e:
            print(f"   ‚ùå File compression failed: {e}")
            return self._calculate_total_size(model_dir)

    def create_distilled_models(self, teacher_model_path: str = "models/dialogue/perfect_faction_model_best.pt"):
        """Create smaller distilled models (requires retraining)"""
        
        print("üéì CREATING DISTILLED STUDENT MODELS")
        print("="*50)
        
        try:
            from transformers import GPT2LMHeadModel, GPT2Config, AutoTokenizer
            import torch.nn as nn
            
            # Load teacher model
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            
            print("üìö Loading teacher model...")
            checkpoint = torch.load(teacher_model_path, map_location=device)
            teacher_model = GPT2LMHeadModel.from_pretrained("gpt2")
            teacher_model.load_state_dict(checkpoint['model_state_dict'])
            teacher_model.to(device)
            teacher_model.eval()
            
            # Create smaller student configuration
            student_config = GPT2Config(
                vocab_size=50257,
                n_positions=512,
                n_embd=384,      # Reduced from 768
                n_layer=6,       # Reduced from 12  
                n_head=6,        # Reduced from 12
                n_inner=1536     # Reduced from 3072
            )
            
            print("üßë‚Äçüéì Creating student model...")
            student_model = GPT2LMHeadModel(student_config)
            student_model.to(device)
            
            # Estimate sizes
            teacher_params = sum(p.numel() for p in teacher_model.parameters())
            student_params = sum(p.numel() for p in student_model.parameters())
            
            print(f"üìä Teacher parameters: {teacher_params:,}")
            print(f"üìä Student parameters: {student_params:,}")
            print(f"üìä Reduction ratio: {student_params/teacher_params:.2f}x")
            print(f"üìä Estimated student size: ~{student_params * 4 / (1024*1024):.0f} MB")
            
            # This would require actual distillation training
            print("\n‚ö†Ô∏è  To complete distillation:")
            print("1. Load your cleaned training data")
            print("2. Train student to match teacher's outputs (distillation loss)")
            print("3. Export distilled models to ONNX")
            print("4. Expected final size: ~150MB per faction (600MB total)")
            
            return True
            
        except Exception as e:
            print(f"‚ùå Distillation setup failed: {e}")
            return False

    def create_streaming_models(self, model_dir: str):
        """Create streaming/chunked models for progressive loading"""
        
        print("üì° CREATING STREAMING MODEL ARCHITECTURE")
        print("="*50)
        
        try:
            # Split models into chunks for streaming
            output_dir = os.path.join(model_dir, "streaming")
            os.makedirs(output_dir, exist_ok=True)
            
            chunk_size_mb = 50  # 50MB chunks
            
            model_files = [f for f in os.listdir(model_dir) if f.endswith('.onnx') and 'dialogue' in f]
            
            for model_file in model_files:
                input_path = os.path.join(model_dir, model_file)
                base_name = model_file.replace('.onnx', '')
                
                print(f"   üì¶ Chunking {model_file}...")
                
                # Read model file
                with open(input_path, 'rb') as f:
                    model_data = f.read()
                
                file_size_mb = len(model_data) / (1024 * 1024)
                num_chunks = int(np.ceil(file_size_mb / chunk_size_mb))
                
                chunk_size_bytes = len(model_data) // num_chunks
                
                # Create chunks
                for i in range(num_chunks):
                    start_idx = i * chunk_size_bytes
                    end_idx = start_idx + chunk_size_bytes if i < num_chunks - 1 else len(model_data)
                    
                    chunk_data = model_data[start_idx:end_idx]
                    chunk_path = os.path.join(output_dir, f"{base_name}_chunk_{i:02d}.bin")
                    
                    with open(chunk_path, 'wb') as f:
                        f.write(chunk_data)
                
                # Create chunk manifest
                manifest = {
                    "model_name": base_name,
                    "total_chunks": num_chunks,
                    "chunk_size_mb": chunk_size_mb,
                    "total_size_mb": file_size_mb,
                    "chunks": [f"{base_name}_chunk_{i:02d}.bin" for i in range(num_chunks)]
                }
                
                manifest_path = os.path.join(output_dir, f"{base_name}_manifest.json")
                with open(manifest_path, 'w') as f:
                    json.dump(manifest, f, indent=2)
                
                print(f"     ‚úÖ Split into {num_chunks} chunks of ~{chunk_size_mb}MB each")
            
            print(f"\nüìã Streaming Implementation Notes:")
            print(f"- Load chunks progressively during game startup")
            print(f"- Reconstruct models in memory from chunks")
            print(f"- Download only needed faction models")
            print(f"- Cache chunks locally for faster subsequent loads")
            
            return True
            
        except Exception as e:
            print(f"‚ùå Streaming model creation failed: {e}")
            return False

def main():
    """Run compression analysis and recommendations"""
    
    print("üéÆ ANGARAKA AI MODEL COMPRESSION ANALYSIS")
    print("="*60)
    
    compressor = ModelCompressor()
    
    # Use the correct directory with faction structure
    model_directory = "models/onnx/shared_optimized"  # This should contain ashvattha/, vaikuntha/, etc.
    
    print(f"üìÅ Looking for models in: {model_directory}")
    
    # Run all compression strategies on the faction structure
    results = compressor.compress_all_strategies(model_directory)
    
    print(f"\nüéØ RECOMMENDATIONS FOR GAME DISTRIBUTION:")
    print(f"="*50)
    
    # Analyze results and make recommendations
    original_size = 600 * 4  # 4 factions √ó 600MB
    
    print(f"\nü•á BEST IMMEDIATE SOLUTION: INT8 Quantization")
    if 'quantization' in results:
        quantized_size = results['quantization']
        print(f"   üìè Size: {original_size:.0f}MB ‚Üí {quantized_size:.0f}MB")
        print(f"   ‚ö° Reduction: {((original_size - quantized_size) / original_size) * 100:.0f}%")
        print(f"   ‚úÖ No retraining required")
        print(f"   ‚úÖ Maintains model quality")
    
    print(f"\nü•à BEST LONG-TERM SOLUTION: Model Distillation")
    if 'distillation' in results:
        distilled_size = results['distillation']
        print(f"   üìè Size: {original_size:.0f}MB ‚Üí {distilled_size:.0f}MB")
        print(f"   ‚ö° Reduction: {((original_size - distilled_size) / original_size) * 100:.0f}%")
        print(f"   ‚ö†Ô∏è Requires retraining")
        print(f"   ‚úÖ Smallest possible size")
    
    print(f"\nü•â ALTERNATIVE: Weight Sharing + Compression")
    if 'weight_sharing' in results and 'compression' in results:
        combined_size = min(results['weight_sharing'], results['compression'])
        print(f"   üìè Size: {original_size:.0f}MB ‚Üí {combined_size:.0f}MB")
        print(f"   ‚ö° Reduction: {((original_size - combined_size) / original_size) * 100:.0f}%")
        print(f"   ‚ö†Ô∏è Requires architecture changes")
    
    print(f"\nüìã IMPLEMENTATION PRIORITY:")
    print(f"1. Apply INT8 quantization (immediate 75% reduction)")
    print(f"2. Add LZMA compression (additional 20-30% reduction)")  
    print(f"3. Consider model distillation for future versions")
    print(f"4. Implement streaming for large models")

if __name__ == "__main__":
    main()